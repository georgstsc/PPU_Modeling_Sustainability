{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing Pipeline\n",
        "\n",
        "**Swiss Energy Portfolio Optimization - Modular Data Pipeline**\n",
        "\n",
        "This notebook provides a software-engineered approach to data preprocessing for energy portfolio optimization.\n",
        "\n",
        "---\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "dumping/          # Raw data files (downloaded externally)\n",
        "  ‚îú‚îÄ‚îÄ solar_incidence/\n",
        "  ‚îú‚îÄ‚îÄ wind_incidence/\n",
        "  ‚îú‚îÄ‚îÄ energy_price_all_2024.json\n",
        "  ‚îú‚îÄ‚îÄ chf_to_eur_2024.csv\n",
        "  ‚îú‚îÄ‚îÄ ...\n",
        "  \n",
        "import/           # Processed data ready for optimization\n",
        "  ‚îú‚îÄ‚îÄ solar_incidence_hourly_2024.csv\n",
        "  ‚îú‚îÄ‚îÄ wind_incidence_hourly_2024.csv\n",
        "  ‚îú‚îÄ‚îÄ spot_price_hourly.csv\n",
        "  ‚îú‚îÄ‚îÄ ...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Pipeline Stages\n",
        "\n",
        "| Stage | Input | Output | Description |\n",
        "|-------|-------|--------|-------------|\n",
        "| **1. Solar** | GRIB files | solar_incidence_hourly_2024.csv | GHI per location |\n",
        "| **2. Wind** | GRIB files | wind_incidence_hourly_2024.csv | Wind speed per location |\n",
        "| **3. Prices** | JSON + CSV | spot_price_hourly.csv | Spot prices in CHF/MWh |\n",
        "| **4. Demand** | CSV | monthly_hourly_load_values_2024.csv | Electricity demand |\n",
        "| **5. Water** | Monthly CSV | water_quarterly_ror_2024.csv | Hydro generation |\n",
        "| **6. PPU** | Excel + Python | ppu_efficiency_lcoe_analysis.csv | Cost analysis |\n",
        "\n",
        "---\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. **Download raw data** into `dumping/` folder (see data sources below)\n",
        "2. **Run this notebook** from top to bottom\n",
        "3. **Processed files** will be saved to `import/` folder\n",
        "4. **Run `Energy_Portfolio_Optimization.ipynb`** for portfolio analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Configuration & Imports\n",
        "\n",
        "All dependencies and configuration in one place following best practices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# IMPORTS & CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Standard library\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import csv\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "\n",
        "# Data handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Scientific computing\n",
        "from scipy.interpolate import CubicSpline\n",
        "from scipy import stats\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from matplotlib.patches import Patch\n",
        "import seaborn as sns\n",
        "\n",
        "# Geospatial data (for GRIB processing)\n",
        "try:\n",
        "    import xarray as xr\n",
        "    HAS_XARRAY = True\n",
        "except ImportError:\n",
        "    HAS_XARRAY = False\n",
        "    print(\"‚ö†Ô∏è xarray not installed. GRIB processing will be skipped.\")\n",
        "    print(\"   Install with: pip install xarray cfgrib\")\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# MATPLOTLIB CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "plt.rcParams['axes.titlesize'] = 13\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "plt.rcParams['xtick.labelsize'] = 10\n",
        "plt.rcParams['ytick.labelsize'] = 10\n",
        "plt.rcParams['legend.fontsize'] = 10\n",
        "\n",
        "# ============================================================================\n",
        "# PATH CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Directories\n",
        "DUMPING_DIR = Path('dumping')  # Raw data\n",
        "IMPORT_DIR = Path('import')    # Processed data\n",
        "PLOT_DIR = IMPORT_DIR / 'plots'  # Visualization outputs\n",
        "\n",
        "# Create directories\n",
        "for d in [DUMPING_DIR, IMPORT_DIR, PLOT_DIR]:\n",
        "    d.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# ============================================================================\n",
        "# GLOBAL CONSTANTS\n",
        "# ============================================================================\n",
        "\n",
        "YEAR = 2024\n",
        "N_HOURS_YEAR = 8784 if YEAR % 4 == 0 else 8760  # 2024 is leap year\n",
        "SWISS_SURFACE_AREA_M2 = 41_285_000_000  # 41,285 km¬≤\n",
        "\n",
        "# ============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def print_section(title: str, char: str = \"=\", width: int = 80) -> None:\n",
        "    \"\"\"Print a formatted section header.\"\"\"\n",
        "    print(\"\\n\" + char * width)\n",
        "    print(title)\n",
        "    print(char * width)\n",
        "\n",
        "def print_subsection(title: str) -> None:\n",
        "    \"\"\"Print a formatted subsection header.\"\"\"\n",
        "    print(f\"\\n{'‚îÄ' * 80}\")\n",
        "    print(f\"  {title}\")\n",
        "    print(f\"{'‚îÄ' * 80}\")\n",
        "\n",
        "def validate_file(path: Path, description: str = \"\") -> bool:\n",
        "    \"\"\"Check if a file exists and print status with file size.\"\"\"\n",
        "    if path.exists():\n",
        "        size_mb = path.stat().st_size / (1024 * 1024)\n",
        "        print(f\"  ‚úÖ {path.name}: {size_mb:.2f} MB\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"  ‚ùå {path.name}: MISSING {f'- {description}' if description else ''}\")\n",
        "        return False\n",
        "\n",
        "def save_plot(fig, filename: str, dpi: int = 150) -> None:\n",
        "    \"\"\"Save a plot to the plot directory.\"\"\"\n",
        "    output_path = PLOT_DIR / filename\n",
        "    fig.savefig(output_path, dpi=dpi, bbox_inches='tight', facecolor='white')\n",
        "    print(f\"  üíæ Saved plot: {output_path.name}\")\n",
        "\n",
        "def print_dataframe_info(df: pd.DataFrame, name: str) -> None:\n",
        "    \"\"\"Print comprehensive DataFrame information.\"\"\"\n",
        "    print(f\"\\nüìä {name}\")\n",
        "    print(f\"  Shape: {df.shape[0]:,} rows √ó {df.shape[1]:,} columns\")\n",
        "    print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    if hasattr(df.index, 'min') and hasattr(df.index, 'max'):\n",
        "        try:\n",
        "            print(f\"  Index range: {df.index.min()} to {df.index.max()}\")\n",
        "        except:\n",
        "            pass\n",
        "    print(f\"  Null values: {df.isnull().sum().sum():,}\")\n",
        "\n",
        "def compute_statistics(data: np.ndarray, name: str) -> Dict[str, float]:\n",
        "    \"\"\"Compute comprehensive statistics for a dataset.\"\"\"\n",
        "    clean_data = data[~np.isnan(data)]\n",
        "    return {\n",
        "        'count': len(clean_data),\n",
        "        'mean': np.mean(clean_data),\n",
        "        'median': np.median(clean_data),\n",
        "        'std': np.std(clean_data),\n",
        "        'min': np.min(clean_data),\n",
        "        'max': np.max(clean_data),\n",
        "        'q25': np.percentile(clean_data, 25),\n",
        "        'q75': np.percentile(clean_data, 75),\n",
        "        'skewness': stats.skew(clean_data),\n",
        "        'kurtosis': stats.kurtosis(clean_data)\n",
        "    }\n",
        "\n",
        "def print_statistics(stats_dict: Dict[str, float], title: str) -> None:\n",
        "    \"\"\"Print formatted statistics.\"\"\"\n",
        "    print(f\"\\nüìà {title}\")\n",
        "    print(f\"  Count:    {stats_dict['count']:,.0f}\")\n",
        "    print(f\"  Mean:     {stats_dict['mean']:.4f}\")\n",
        "    print(f\"  Median:   {stats_dict['median']:.4f}\")\n",
        "    print(f\"  Std:      {stats_dict['std']:.4f}\")\n",
        "    print(f\"  Min:      {stats_dict['min']:.4f}\")\n",
        "    print(f\"  Max:      {stats_dict['max']:.4f}\")\n",
        "    print(f\"  Q25:      {stats_dict['q25']:.4f}\")\n",
        "    print(f\"  Q75:      {stats_dict['q75']:.4f}\")\n",
        "    print(f\"  Skewness: {stats_dict['skewness']:.4f}\")\n",
        "    print(f\"  Kurtosis: {stats_dict['kurtosis']:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STATUS\n",
        "# ============================================================================\n",
        "\n",
        "print_section(\"DATA PREPROCESSING PIPELINE - SWISS ENERGY PORTFOLIO OPTIMIZATION\")\n",
        "print(f\"‚úÖ All imports successful\")\n",
        "print(f\"   Year: {YEAR} ({N_HOURS_YEAR} hours)\")\n",
        "print(f\"   Dumping directory: {DUMPING_DIR.absolute()}\")\n",
        "print(f\"   Import directory: {IMPORT_DIR.absolute()}\")\n",
        "print(f\"   Plot directory: {PLOT_DIR.absolute()}\")\n",
        "print(f\"   xarray available: {HAS_XARRAY}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Solar Incidence Data\n",
        "\n",
        "**Source**: ERA5 reanalysis GRIB files  \n",
        "**Input**: `dumping/solar_incidence/*.grib`  \n",
        "**Output**: `import/solar_incidence_hourly_2024.csv`, `import/solar_incidence_ranking.csv`  \n",
        "**Processing**: Extract SSRD, convert J/m¬≤ to kWh/m¬≤, pivot to hourly √ó locations format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Data Treatment: GRIB to CSV Conversion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 1.1 SOLAR: GRIB TO CSV CONVERSION\n",
        "# ============================================================================\n",
        "\n",
        "def process_solar_grib_to_csv(input_dir: Path, output_csv: Path) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Process solar irradiance GRIB files into hourly CSV.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_dir : Path\n",
        "        Directory containing GRIB files\n",
        "    output_csv : Path\n",
        "        Output CSV path\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame or None\n",
        "        Pivoted DataFrame with hourly solar incidence per location (hours √ó locations)\n",
        "        Returns None if processing fails\n",
        "    \"\"\"\n",
        "    if not HAS_XARRAY:\n",
        "        print(\"  ‚ö†Ô∏è xarray not available - cannot process GRIB files\")\n",
        "        return None\n",
        "    \n",
        "    print_subsection(\"Processing Solar GRIB Files\")\n",
        "    \n",
        "    # Find all GRIB files (exclude .idx index files)\n",
        "    grib_files = sorted([p for p in input_dir.rglob(\"*\") \n",
        "                         if p.is_file() and not p.name.endswith('.idx')])\n",
        "    \n",
        "    if not grib_files:\n",
        "        print(f\"  ‚ùå No GRIB files found in {input_dir}\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"  üìÇ Found {len(grib_files)} GRIB files\")\n",
        "    \n",
        "    # Load all datasets\n",
        "    all_dsets = []\n",
        "    for f in grib_files:\n",
        "        try:\n",
        "            ds = xr.open_dataset(\n",
        "                f,\n",
        "                engine=\"cfgrib\",\n",
        "                backend_kwargs={\"filter_by_keys\": {\"shortName\": \"ssrd\"}},\n",
        "            )\n",
        "            all_dsets.append(ds)\n",
        "            print(f\"     ‚úì Loaded {f.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"     ‚ö†Ô∏è Failed to open {f.name}: {e}\")\n",
        "    \n",
        "    if not all_dsets:\n",
        "        print(\"  ‚ùå No GRIB datasets could be opened\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"  üîÑ Combining {len(all_dsets)} datasets...\")\n",
        "    \n",
        "    # Combine all months\n",
        "    ds_all = xr.combine_by_coords(all_dsets, combine_attrs=\"override\")\n",
        "    \n",
        "    # Find SSRD variable (Surface Solar Radiation Downwards)\n",
        "    var_name = next((v for v in ds_all.data_vars \n",
        "                     if v.lower() in (\"ssrd\", \"surface_solar_radiation_downwards\")), None)\n",
        "    if var_name is None:\n",
        "        var_name = next((v for v in ds_all.data_vars if \"ssrd\" in v.lower()), None)\n",
        "    if var_name is None:\n",
        "        print(f\"  ‚ùå No SSRD variable found. Variables: {list(ds_all.data_vars)}\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"  ‚öôÔ∏è  Found variable: {var_name}\")\n",
        "    print(f\"  ‚öôÔ∏è  Converting J/m¬≤ to kWh/m¬≤...\")\n",
        "    \n",
        "    # Convert J/m¬≤ to kWh/m¬≤\n",
        "    ghi_kwh = ds_all[var_name] / 3_600_000.0  # J/m¬≤ ‚Üí kWh/m¬≤\n",
        "    \n",
        "    # Round coordinates for numerical stability\n",
        "    ghi_kwh['latitude'] = np.round(ghi_kwh['latitude'].values, 1)\n",
        "    ghi_kwh['longitude'] = np.round(ghi_kwh['longitude'].values, 1)\n",
        "    \n",
        "    print(f\"  ‚öôÔ∏è  Pivoting to wide format...\")\n",
        "    \n",
        "    # Convert to DataFrame and pivot\n",
        "    df = ghi_kwh.to_dataframe()\n",
        "    df_pivot = df.pivot_table(index=\"time\", columns=[\"latitude\", \"longitude\"], values=var_name)\n",
        "    df_pivot.index.name = \"time\"\n",
        "    \n",
        "    # Save to CSV\n",
        "    df_pivot.to_csv(output_csv)\n",
        "    \n",
        "    print(f\"  ‚úÖ Saved solar incidence to {output_csv.name}\")\n",
        "    print(f\"     Shape: {df_pivot.shape[0]:,} hours √ó {df_pivot.shape[1]:,} locations\")\n",
        "    print(f\"     Date range: {df_pivot.index.min()} to {df_pivot.index.max()}\")\n",
        "    \n",
        "    return df_pivot\n",
        "\n",
        "\n",
        "def create_solar_ranking(solar_csv: Path, output_csv: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Rank locations by mean solar incidence (best locations first).\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    solar_csv : Path\n",
        "        Path to solar incidence CSV\n",
        "    output_csv : Path\n",
        "        Output ranking CSV path\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Ranking DataFrame with lat, lon, mean_solar_incidence, rank\n",
        "    \"\"\"\n",
        "    print_subsection(\"Creating Solar Location Ranking\")\n",
        "    \n",
        "    # Read header rows to get lat/lon\n",
        "    with open(solar_csv, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        lat_row = next(reader)\n",
        "        lon_row = next(reader)\n",
        "    \n",
        "    # Read data (skip headers)\n",
        "    df = pd.read_csv(solar_csv, skiprows=3)\n",
        "    df.rename(columns={df.columns[0]: \"time\"}, inplace=True)\n",
        "    data_cols = [col for col in df.columns if col != \"time\"]\n",
        "    \n",
        "    # Compute mean per location\n",
        "    mean_by_loc = df[data_cols].mean()\n",
        "    \n",
        "    # Extract coordinates\n",
        "    lats = [float(x) for x in lat_row[1:]]\n",
        "    lons = [float(x) for x in lon_row[1:]]\n",
        "    \n",
        "    # Create ranking DataFrame\n",
        "    ranking_df = pd.DataFrame({\n",
        "        'latitude': lats,\n",
        "        'longitude': lons,\n",
        "        'mean_solar_incidence_kwh_m2_per_hour': mean_by_loc.values\n",
        "    })\n",
        "    ranking_df = ranking_df.sort_values('mean_solar_incidence_kwh_m2_per_hour', \n",
        "                                         ascending=False).reset_index(drop=True)\n",
        "    ranking_df['rank'] = ranking_df.index + 1\n",
        "    \n",
        "    # Save\n",
        "    ranking_df.to_csv(output_csv, index=False)\n",
        "    \n",
        "    print(f\"  ‚úÖ Saved ranking to {output_csv.name}\")\n",
        "    print(f\"\\n  üèÜ Top 5 Solar Locations:\")\n",
        "    for _, row in ranking_df.head(5).iterrows():\n",
        "        print(f\"     {int(row['rank'])}. ({row['latitude']:.1f}, {row['longitude']:.1f}) - \"\n",
        "              f\"{row['mean_solar_incidence_kwh_m2_per_hour']:.4f} kWh/m¬≤/h\")\n",
        "    \n",
        "    return ranking_df\n",
        "\n",
        "\n",
        "# Process solar data\n",
        "solar_input = DUMPING_DIR / 'solar_incidence'\n",
        "solar_output = IMPORT_DIR / 'solar_incidence_hourly_2024.csv'\n",
        "solar_ranking_output = IMPORT_DIR / 'solar_incidence_ranking.csv'\n",
        "\n",
        "print_section(\"1. SOLAR INCIDENCE DATA PROCESSING\")\n",
        "\n",
        "if solar_input.exists() and HAS_XARRAY:\n",
        "    try:\n",
        "        solar_df = process_solar_grib_to_csv(solar_input, solar_output)\n",
        "        if solar_df is not None:\n",
        "            solar_ranking = create_solar_ranking(solar_output, solar_ranking_output)\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è Solar processing failed: {e}\")\n",
        "        print(f\"     Attempting to load existing CSV...\")\n",
        "        if solar_output.exists():\n",
        "            solar_df = pd.read_csv(solar_output, index_col=0, header=[0,1], parse_dates=True)\n",
        "            print(f\"  ‚úÖ Loaded existing: {solar_df.shape}\")\n",
        "        else:\n",
        "            solar_df = None\n",
        "elif solar_output.exists():\n",
        "    print_subsection(\"Loading Existing Solar Data\")\n",
        "    solar_df = pd.read_csv(solar_output, index_col=0, header=[0,1], parse_dates=True)\n",
        "    print(f\"  ‚úÖ Loaded: {solar_df.shape[0]:,} hours √ó {solar_df.shape[1]:,} locations\")\n",
        "    if solar_ranking_output.exists():\n",
        "        solar_ranking = pd.read_csv(solar_ranking_output)\n",
        "        print(f\"  ‚úÖ Loaded ranking: {len(solar_ranking):,} locations\")\n",
        "else:\n",
        "    print(\"  ‚ùå No solar data available. Download GRIB files to dumping/solar_incidence/\")\n",
        "    solar_df = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Preprocessing Plots: Solar Analysis\n",
        "\n",
        "Statistical analysis and visualization of solar incidence data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 1.2 SOLAR: PREPROCESSING PLOTS & ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "if 'solar_df' in globals() and solar_df is not None:\n",
        "    print_section(\"1.2 SOLAR DATA ANALYSIS\")\n",
        "    \n",
        "    # Statistics\n",
        "    solar_values = solar_df.values.flatten()\n",
        "    solar_values = solar_values[~np.isnan(solar_values)]\n",
        "    solar_stats = compute_statistics(solar_values, \"Solar\")\n",
        "    print_statistics(solar_stats, \"Solar Incidence (kWh/m¬≤/hour)\")\n",
        "    \n",
        "    # Visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "    \n",
        "    # Monthly average\n",
        "    monthly_avg = solar_df.mean(axis=1).resample('M').mean()\n",
        "    axes[0, 0].bar(range(1, 13), monthly_avg.values, color='orange', alpha=0.7)\n",
        "    axes[0, 0].set_title('Monthly Average Solar Incidence')\n",
        "    axes[0, 0].set_xlabel('Month')\n",
        "    axes[0, 0].set_ylabel('kWh/m¬≤/hour')\n",
        "    \n",
        "    # Distribution\n",
        "    axes[0, 1].hist(solar_values[solar_values > 0], bins=50, color='orange', alpha=0.7)\n",
        "    axes[0, 1].set_title('Distribution (Non-Zero Values)')\n",
        "    axes[0, 1].set_xlabel('kWh/m¬≤/hour')\n",
        "    \n",
        "    # Sample day profile\n",
        "    sample_day = solar_df.loc['2024-07-15':'2024-07-16'].mean(axis=1)\n",
        "    axes[1, 0].plot(sample_day.index.hour, sample_day.values, 'o-', color='orange')\n",
        "    axes[1, 0].set_title('Sample Day Profile (2024-07-15)')\n",
        "    axes[1, 0].set_xlabel('Hour')\n",
        "    \n",
        "    # Annual variation\n",
        "    daily_mean = solar_df.mean(axis=1).resample('D').mean()\n",
        "    axes[1, 1].plot(daily_mean.index, daily_mean.values, linewidth=0.5, color='orange')\n",
        "    axes[1, 1].set_title('Daily Mean Solar Incidence Over Year')\n",
        "    axes[1, 1].set_xlabel('Date')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    save_plot(fig, '01_solar_analysis.png')\n",
        "    plt.show()\n",
        "    print(\"‚úÖ Solar analysis complete\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Solar data not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Wind Incidence Data\n",
        "\n",
        "**Source**: ERA5 reanalysis GRIB files  \n",
        "**Input**: `dumping/wind_incidence/*.grib`  \n",
        "**Output**: `import/wind_incidence_hourly_2024.csv`, `import/wind_incidence_ranking.csv`  \n",
        "**Processing**: Extract u10/v10 components, calculate wind speed, pivot to hourly √ó locations format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Data Treatment: Wind GRIB to CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 2.1 WIND: GRIB TO CSV CONVERSION\n",
        "# ============================================================================\n",
        "\n",
        "def process_wind_grib_to_csv(input_dir: Path, output_csv: Path) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Process wind GRIB files: extract u10, v10 and compute wind speed.\"\"\"\n",
        "    if not HAS_XARRAY:\n",
        "        print(\"  ‚ö†Ô∏è xarray not available\")\n",
        "        return None\n",
        "    \n",
        "    print_subsection(\"Processing Wind GRIB Files\")\n",
        "    \n",
        "    grib_files = sorted([p for p in input_dir.rglob(\"*\") \n",
        "                         if p.is_file() and not p.name.endswith('.idx')])\n",
        "    \n",
        "    if not grib_files:\n",
        "        print(f\"  ‚ùå No GRIB files found\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"  üìÇ Found {len(grib_files)} GRIB files\")\n",
        "    \n",
        "    all_dfs = []\n",
        "    for f in grib_files:\n",
        "        try:\n",
        "            ds = xr.open_dataset(f, engine=\"cfgrib\")\n",
        "            wind_speed = np.sqrt(ds[\"u10\"]**2 + ds[\"v10\"]**2)\n",
        "            if \"valid_time\" in ds:\n",
        "                wind_speed = wind_speed.assign_coords(time=ds[\"valid_time\"])\n",
        "            df = wind_speed.to_dataframe(name=\"wind_speed\").reset_index()\n",
        "            all_dfs.append(df)\n",
        "            print(f\"     ‚úì Loaded {f.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"     ‚ö†Ô∏è Failed {f.name}: {e}\")\n",
        "    \n",
        "    if not all_dfs:\n",
        "        return None\n",
        "    \n",
        "    # Concatenate\n",
        "    wind_data = pd.concat(all_dfs, ignore_index=True)\n",
        "    wind_data = wind_data.rename(columns={\"valid_time\": \"datetime\"})\n",
        "    wind_data = wind_data.dropna(subset=[\"wind_speed\", \"datetime\"])\n",
        "    \n",
        "    # Round coordinates\n",
        "    wind_data[\"latitude\"] = wind_data[\"latitude\"].round(1)\n",
        "    wind_data[\"longitude\"] = wind_data[\"longitude\"].round(1)\n",
        "    \n",
        "    # Pivot\n",
        "    wind_pivot = wind_data.pivot_table(\n",
        "        index=\"datetime\", \n",
        "        columns=[\"latitude\", \"longitude\"], \n",
        "        values=\"wind_speed\"\n",
        "    )\n",
        "    wind_pivot.index.name = \"datetime\"\n",
        "    \n",
        "    # Save\n",
        "    wind_pivot.to_csv(output_csv)\n",
        "    print(f\"  ‚úÖ Saved to {output_csv.name}\")\n",
        "    print(f\"     Shape: {wind_pivot.shape[0]:,} √ó {wind_pivot.shape[1]:,}\")\n",
        "    \n",
        "    return wind_pivot\n",
        "\n",
        "def create_wind_ranking(wind_csv: Path, output_csv: Path) -> pd.DataFrame:\n",
        "    \"\"\"Rank wind locations by mean speed.\"\"\"\n",
        "    print_subsection(\"Creating Wind Location Ranking\")\n",
        "    \n",
        "    df = pd.read_csv(wind_csv, index_col=0, header=[0,1], parse_dates=True)\n",
        "    mean_by_loc = df.mean()\n",
        "    \n",
        "    lats = [round(float(col[0]), 1) for col in df.columns]\n",
        "    lons = [round(float(col[1]), 1) for col in df.columns]\n",
        "    \n",
        "    ranking_df = pd.DataFrame({\n",
        "        'latitude': lats,\n",
        "        'longitude': lons,\n",
        "        'mean_wind_speed_m_per_s': mean_by_loc.values\n",
        "    })\n",
        "    ranking_df = ranking_df.sort_values('mean_wind_speed_m_per_s', ascending=False).reset_index(drop=True)\n",
        "    ranking_df['rank'] = ranking_df.index + 1\n",
        "    ranking_df.to_csv(output_csv, index=False)\n",
        "    \n",
        "    print(f\"  ‚úÖ Saved ranking\")\n",
        "    print(f\"\\n  üèÜ Top 5 Wind Locations:\")\n",
        "    for _, row in ranking_df.head(5).iterrows():\n",
        "        print(f\"     {int(row['rank'])}. ({row['latitude']:.1f}, {row['longitude']:.1f}) - \"\n",
        "              f\"{row['mean_wind_speed_m_per_s']:.2f} m/s\")\n",
        "    \n",
        "    return ranking_df\n",
        "\n",
        "# Process wind data\n",
        "wind_input = DUMPING_DIR / 'wind_incidence'\n",
        "wind_output = IMPORT_DIR / 'wind_incidence_hourly_2024.csv'\n",
        "wind_ranking_output = IMPORT_DIR / 'wind_incidence_ranking.csv'\n",
        "\n",
        "print_section(\"2. WIND INCIDENCE DATA PROCESSING\")\n",
        "\n",
        "if wind_input.exists() and HAS_XARRAY:\n",
        "    try:\n",
        "        wind_df = process_wind_grib_to_csv(wind_input, wind_output)\n",
        "        if wind_df is not None:\n",
        "            wind_ranking = create_wind_ranking(wind_output, wind_ranking_output)\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è Wind processing failed: {e}\")\n",
        "        if wind_output.exists():\n",
        "            wind_df = pd.read_csv(wind_output, index_col=0, header=[0,1], parse_dates=True)\n",
        "            print(f\"  ‚úÖ Loaded existing\")\n",
        "        else:\n",
        "            wind_df = None\n",
        "elif wind_output.exists():\n",
        "    print_subsection(\"Loading Existing Wind Data\")\n",
        "    wind_df = pd.read_csv(wind_output, index_col=0, header=[0,1], parse_dates=True)\n",
        "    print(f\"  ‚úÖ Loaded: {wind_df.shape}\")\n",
        "else:\n",
        "    print(\"  ‚ùå No wind data available\")\n",
        "    wind_df = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Preprocessing Plots: Wind Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 2.2 WIND: PREPROCESSING PLOTS\n",
        "# ============================================================================\n",
        "\n",
        "if 'wind_df' in globals() and wind_df is not None:\n",
        "    print_section(\"2.2 WIND DATA ANALYSIS\")\n",
        "    \n",
        "    wind_values = wind_df.values.flatten()\n",
        "    wind_values = wind_values[~np.isnan(wind_values)]\n",
        "    wind_stats = compute_statistics(wind_values, \"Wind\")\n",
        "    print_statistics(wind_stats, \"Wind Speed (m/s)\")\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "    \n",
        "    # Monthly average\n",
        "    monthly_avg = wind_df.mean(axis=1).resample('M').mean()\n",
        "    axes[0, 0].bar(range(1, 13), monthly_avg.values, color='steelblue', alpha=0.7)\n",
        "    axes[0, 0].set_title('Monthly Average Wind Speed')\n",
        "    axes[0, 0].set_xlabel('Month')\n",
        "    axes[0, 0].set_ylabel('m/s')\n",
        "    \n",
        "    # Distribution\n",
        "    axes[0, 1].hist(wind_values, bins=50, color='steelblue', alpha=0.7)\n",
        "    axes[0, 1].set_title('Wind Speed Distribution')\n",
        "    axes[0, 1].set_xlabel('m/s')\n",
        "    \n",
        "    # Sample day\n",
        "    sample_day = wind_df.loc['2024-03-15':'2024-03-16'].mean(axis=1)\n",
        "    axes[1, 0].plot(sample_day.index.hour, sample_day.values, 'o-', color='steelblue')\n",
        "    axes[1, 0].set_title('Sample Day Profile (2024-03-15)')\n",
        "    axes[1, 0].set_xlabel('Hour')\n",
        "    axes[1, 0].set_ylabel('m/s')\n",
        "    \n",
        "    # Annual variation\n",
        "    daily_mean = wind_df.mean(axis=1).resample('D').mean()\n",
        "    axes[1, 1].plot(daily_mean.index, daily_mean.values, linewidth=0.5, color='steelblue')\n",
        "    axes[1, 1].set_title('Daily Mean Wind Speed')\n",
        "    axes[1, 1].set_xlabel('Date')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    save_plot(fig, '02_wind_analysis.png')\n",
        "    plt.show()\n",
        "    print(\"‚úÖ Wind analysis complete\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Wind data not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Spot Prices & Exchange Rates\n",
        "\n",
        "**Sources**: ENTSO-E (prices), ECB (EUR/CHF), Dukascopy (USD/CHF)  \n",
        "**Input**: `dumping/energy_price_all_2024.json`, `dumping/chf_to_eur_2024.csv`, `dumping/DAT_ASCII_USDCHF_M1_2024.csv`  \n",
        "**Output**: `import/spot_price_hourly.csv`, `import/chf_to_eur_2024.csv`  \n",
        "**Processing**: Convert prices to CHF/MWh, handle timezones\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Data Treatment: Spot Prices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 3.1 SPOT PRICES: JSON TO CSV\n",
        "# ============================================================================\n",
        "\n",
        "print_section(\"3. SPOT PRICES & EXCHANGE RATES\")\n",
        "\n",
        "# Process spot prices\n",
        "price_input = DUMPING_DIR / 'energy_price_all_2024.json'\n",
        "price_output = IMPORT_DIR / 'spot_price_hourly.csv'\n",
        "\n",
        "if price_input.exists():\n",
        "    print_subsection(\"Processing Spot Prices\")\n",
        "    \n",
        "    with open(price_input, 'r') as f:\n",
        "        price_data = json.load(f)\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    price_df = pd.DataFrame([\n",
        "        {'time': pd.to_datetime(entry['time']), 'price': float(entry['price'])}\n",
        "        for entry in price_data\n",
        "    ])\n",
        "    price_df = price_df.set_index('time').sort_index()\n",
        "    \n",
        "    # Save\n",
        "    price_df.to_csv(price_output)\n",
        "    print(f\"  ‚úÖ Saved {len(price_df):,} hourly price points\")\n",
        "    print(f\"     Range: {price_df.index.min()} to {price_df.index.max()}\")\n",
        "    print(f\"     Price range: {price_df['price'].min():.2f} - {price_df['price'].max():.2f} CHF/MWh\")\n",
        "    \n",
        "elif price_output.exists():\n",
        "    price_df = pd.read_csv(price_output, index_col=0, parse_dates=True)\n",
        "    print(f\"  ‚úÖ Loaded existing prices: {len(price_df):,} points\")\n",
        "else:\n",
        "    print(\"  ‚ùå No price data available\")\n",
        "    price_df = None\n",
        "\n",
        "# Process exchange rates\n",
        "exchange_input = DUMPING_DIR / 'chf_to_eur_2024.csv'\n",
        "exchange_output = IMPORT_DIR / 'chf_to_eur_2024.csv'\n",
        "\n",
        "if exchange_input.exists():\n",
        "    print_subsection(\"Processing Exchange Rates\")\n",
        "    \n",
        "    exchange_df = pd.read_csv(exchange_input)\n",
        "    exchange_df.to_csv(exchange_output, index=False)\n",
        "    print(f\"  ‚úÖ Saved exchange rates: {len(exchange_df)} days\")\n",
        "    \n",
        "    if 'CHF to EUR' in exchange_df.columns:\n",
        "        rate_col = 'CHF to EUR'\n",
        "        print(f\"     Mean rate: {exchange_df[rate_col].mean():.4f}\")\n",
        "        print(f\"     Range: {exchange_df[rate_col].min():.4f} - {exchange_df[rate_col].max():.4f}\")\n",
        "        \n",
        "elif exchange_output.exists():\n",
        "    exchange_df = pd.read_csv(exchange_output)\n",
        "    print(f\"  ‚úÖ Loaded existing exchange rates\")\n",
        "else:\n",
        "    print(\"  ‚ùå No exchange rate data\")\n",
        "    exchange_df = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Preprocessing Plots: Price Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 3.2 SPOT PRICES: ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "if 'price_df' in globals() and price_df is not None:\n",
        "    print_section(\"3.2 SPOT PRICE ANALYSIS\")\n",
        "    \n",
        "    price_values = price_df['price'].values\n",
        "    price_stats = compute_statistics(price_values, \"Price\")\n",
        "    print_statistics(price_stats, \"Spot Prices (CHF/MWh)\")\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "    \n",
        "    # Time series\n",
        "    axes[0, 0].plot(price_df.index, price_df['price'], linewidth=0.5, color='darkgreen')\n",
        "    axes[0, 0].set_title('Hourly Spot Prices (2024)')\n",
        "    axes[0, 0].set_ylabel('CHF/MWh')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Distribution\n",
        "    axes[0, 1].hist(price_values, bins=50, color='darkgreen', alpha=0.7)\n",
        "    axes[0, 1].set_title('Price Distribution')\n",
        "    axes[0, 1].set_xlabel('CHF/MWh')\n",
        "    axes[0, 1].axvline(price_stats['mean'], color='red', linestyle='--', label='Mean')\n",
        "    axes[0, 1].legend()\n",
        "    \n",
        "    # Monthly average\n",
        "    monthly_avg = price_df.resample('M').mean()\n",
        "    axes[1, 0].bar(range(1, len(monthly_avg) + 1), monthly_avg['price'].values, \n",
        "                   color='darkgreen', alpha=0.7)\n",
        "    axes[1, 0].set_title('Monthly Average Prices')\n",
        "    axes[1, 0].set_xlabel('Month')\n",
        "    axes[1, 0].set_ylabel('CHF/MWh')\n",
        "    \n",
        "    # Daily profile\n",
        "    price_df['hour'] = price_df.index.hour\n",
        "    hourly_avg = price_df.groupby('hour')['price'].mean()\n",
        "    axes[1, 1].plot(hourly_avg.index, hourly_avg.values, 'o-', color='darkgreen', linewidth=2)\n",
        "    axes[1, 1].set_title('Average Daily Price Profile')\n",
        "    axes[1, 1].set_xlabel('Hour of Day')\n",
        "    axes[1, 1].set_ylabel('CHF/MWh')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    save_plot(fig, '03_spot_price_analysis.png')\n",
        "    plt.show()\n",
        "    print(\"‚úÖ Price analysis complete\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Price data not available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Demand & Water Data\n",
        "\n",
        "**Sources**: ENTSO-E (demand), BFE (hydro), Open-Meteo (water inflow)  \n",
        "**Input**: `dumping/monthly_hourly_load_values_2024.csv`, `dumping/water_monthly_ror_2024.csv`, `dumping/Swiss_Water_Hourly_2024.csv`  \n",
        "**Output**: `import/monthly_hourly_load_values_2024.csv`, `import/water_quarterly_ror_2024.csv`  \n",
        "**Processing**: Load demand, disaggregate monthly RoR to 15-min intervals via cubic spline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 4. DEMAND & WATER DATA\n",
        "# ============================================================================\n",
        "\n",
        "print_section(\"4. DEMAND & WATER DATA PROCESSING\")\n",
        "\n",
        "# Demand data\n",
        "demand_input = DUMPING_DIR / 'monthly_hourly_load_values_2024.csv'\n",
        "demand_output = IMPORT_DIR / 'monthly_hourly_load_values_2024.csv'\n",
        "\n",
        "if demand_input.exists():\n",
        "    print_subsection(\"Processing Demand Data\")\n",
        "    demand_df = pd.read_csv(demand_input)\n",
        "    demand_df.to_csv(demand_output, index=False)\n",
        "    print(f\"  ‚úÖ Saved demand data: {len(demand_df):,} entries\")\n",
        "    \n",
        "    if 'Value' in demand_df.columns:\n",
        "        print(f\"     Mean demand: {demand_df['Value'].mean():.2f} MW\")\n",
        "        print(f\"     Peak demand: {demand_df['Value'].max():.2f} MW\")\n",
        "elif demand_output.exists():\n",
        "    demand_df = pd.read_csv(demand_output)\n",
        "    print(f\"  ‚úÖ Loaded demand: {len(demand_df):,} entries\")\n",
        "else:\n",
        "    print(\"  ‚ùå No demand data\")\n",
        "    demand_df = None\n",
        "\n",
        "# Water RoR data - disaggregate monthly to 15-min\n",
        "ror_input = DUMPING_DIR / 'water_monthly_ror_2024.csv'\n",
        "ror_output = IMPORT_DIR / 'water_quarterly_ror_2024.csv'\n",
        "\n",
        "if ror_input.exists():\n",
        "    print_subsection(\"Processing Run-of-River Data\")\n",
        "    \n",
        "    monthly_ror = pd.read_csv(ror_input)\n",
        "    monthly_ror['Month'] = pd.to_datetime(monthly_ror['Month'])\n",
        "    monthly_ror = monthly_ror.set_index('Month')\n",
        "    \n",
        "    # Convert GWh to MW\n",
        "    monthly_ror['hours_in_month'] = monthly_ror.index.days_in_month * 24\n",
        "    monthly_ror['avg_power_MW'] = (monthly_ror['RoR_GWh'] / monthly_ror['hours_in_month']) * 1000\n",
        "    \n",
        "    # Create interpolation points\n",
        "    start_of_year = pd.Timestamp('2024-01-01')\n",
        "    month_midpoints = []\n",
        "    power_values = []\n",
        "    \n",
        "    for idx, row in monthly_ror.iterrows():\n",
        "        mid_day = idx + pd.Timedelta(days=idx.days_in_month/2)\n",
        "        month_midpoints.append(mid_day)\n",
        "        power_values.append(row['avg_power_MW'])\n",
        "    \n",
        "    x_days = np.array([(d - start_of_year).total_seconds() / 86400 for d in month_midpoints])\n",
        "    y_power = np.array(power_values)\n",
        "    \n",
        "    # Cubic spline interpolation\n",
        "    x_extended = np.concatenate([[x_days[0] - 365], x_days, [x_days[-1] + 365]])\n",
        "    y_extended = np.concatenate([[y_power[-1]], y_power, [y_power[0]]])\n",
        "    cs = CubicSpline(x_extended, y_extended, bc_type='natural')\n",
        "    \n",
        "    # 15-minute timestamps\n",
        "    timestamps_15min = pd.date_range(start='2024-01-01 00:00:00', \n",
        "                                     end='2024-12-31 23:45:00', freq='15min')\n",
        "    x_15min = np.array([(t - start_of_year).total_seconds() / 86400 for t in timestamps_15min])\n",
        "    power_15min_MW = np.maximum(cs(x_15min), 0)\n",
        "    \n",
        "    # Save\n",
        "    df_ror_15min = pd.DataFrame({\n",
        "        'timestamp': timestamps_15min,\n",
        "        'RoR_MW': power_15min_MW\n",
        "    })\n",
        "    df_ror_15min.to_csv(ror_output, index=False)\n",
        "    \n",
        "    print(f\"  ‚úÖ Saved 15-min RoR: {len(df_ror_15min):,} timesteps\")\n",
        "    print(f\"     Power range: {power_15min_MW.min():.2f} - {power_15min_MW.max():.2f} MW\")\n",
        "    \n",
        "    # Energy conservation check\n",
        "    energy_15min = (power_15min_MW * 0.25 / 1000).sum()  # 15min = 0.25h\n",
        "    energy_original = monthly_ror['RoR_GWh'].sum()\n",
        "    print(f\"     Energy check: {energy_original:.2f} GWh (original) vs {energy_15min:.2f} GWh (interpolated)\")\n",
        "    print(f\"     Error: {abs(energy_15min - energy_original) / energy_original * 100:.3f}%\")\n",
        "    \n",
        "elif ror_output.exists():\n",
        "    df_ror_15min = pd.read_csv(ror_output)\n",
        "    print(f\"  ‚úÖ Loaded RoR: {len(df_ror_15min):,} timesteps\")\n",
        "else:\n",
        "    print(\"  ‚ùå No RoR data\")\n",
        "    df_ror_15min = None\n",
        "\n",
        "# Water hourly data\n",
        "water_input = DUMPING_DIR / 'Swiss_Water_Hourly_2024.csv'\n",
        "water_output = IMPORT_DIR / 'Swiss_Water_Hourly_2024.csv'\n",
        "\n",
        "if water_input.exists():\n",
        "    print_subsection(\"Processing Water Inflow Data\")\n",
        "    water_df = pd.read_csv(water_input)\n",
        "    water_df.to_csv(water_output, index=False)\n",
        "    print(f\"  ‚úÖ Saved water data: {len(water_df):,} hours\")\n",
        "elif water_output.exists():\n",
        "    water_df = pd.read_csv(water_output)\n",
        "    print(f\"  ‚úÖ Loaded water: {len(water_df):,} hours\")\n",
        "else:\n",
        "    print(\"  ‚ùå No water data\")\n",
        "    water_df = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. PPU Cost Analysis\n",
        "\n",
        "**Source**: Cost table and analyze_ppu_chains.py logic  \n",
        "**Input**: `dumping/cost_table_tidy.csv`, `dumping/ppu_constructs_components.csv`  \n",
        "**Output**: `import/cost_table_tidy.csv`, `import/ppu_constructs_components.csv`, `import/ppu_efficiency_lcoe_analysis.csv`  \n",
        "**Processing**: Copy cost tables, run PPU chain analysis for efficiency and LCOE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 5. PPU COST ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "print_section(\"5. PPU COST ANALYSIS\")\n",
        "\n",
        "# Copy cost table\n",
        "cost_input = DUMPING_DIR / 'cost_table_tidy.csv'\n",
        "cost_output = IMPORT_DIR / 'cost_table_tidy.csv'\n",
        "\n",
        "if cost_input.exists():\n",
        "    print_subsection(\"Processing Cost Table\")\n",
        "    cost_df = pd.read_csv(cost_input)\n",
        "    cost_df.to_csv(cost_output, index=False)\n",
        "    print(f\"  ‚úÖ Saved cost table: {len(cost_df)} components\")\n",
        "elif cost_output.exists():\n",
        "    cost_df = pd.read_csv(cost_output)\n",
        "    print(f\"  ‚úÖ Loaded cost table: {len(cost_df)} components\")\n",
        "else:\n",
        "    print(\"  ‚ùå No cost table\")\n",
        "    cost_df = None\n",
        "\n",
        "# Copy PPU constructs\n",
        "ppu_input = DUMPING_DIR / 'ppu_constructs_components.csv'\n",
        "ppu_output = IMPORT_DIR / 'ppu_constructs_components.csv'\n",
        "\n",
        "if ppu_input.exists():\n",
        "    print_subsection(\"Processing PPU Constructs\")\n",
        "    ppu_df = pd.read_csv(ppu_input)\n",
        "    ppu_df.to_csv(ppu_output, index=False)\n",
        "    print(f\"  ‚úÖ Saved PPU constructs: {len(ppu_df)} PPUs\")\n",
        "elif ppu_output.exists():\n",
        "    ppu_df = pd.read_csv(ppu_output)\n",
        "    print(f\"  ‚úÖ Loaded PPU constructs: {len(ppu_df)} PPUs\")\n",
        "else:\n",
        "    print(\"  ‚ùå No PPU constructs\")\n",
        "    ppu_df = None\n",
        "\n",
        "# Run PPU efficiency analysis (simplified version)\n",
        "if cost_df is not None and ppu_df is not None:\n",
        "    print_subsection(\"Running PPU Efficiency Analysis\")\n",
        "    \n",
        "    # This is a simplified placeholder - in practice, run analyze_ppu_chains.py\n",
        "    print(\"  ‚öôÔ∏è  PPU efficiency analysis should be run separately using analyze_ppu_chains.py\")\n",
        "    print(\"     This script computes LCOE and efficiency for all PPU chains\")\n",
        "    \n",
        "    # Check if analysis already exists\n",
        "    ppu_analysis_output = IMPORT_DIR / 'ppu_efficiency_lcoe_analysis.csv'\n",
        "    if ppu_analysis_output.exists():\n",
        "        ppu_analysis = pd.read_csv(ppu_analysis_output)\n",
        "        print(f\"  ‚úÖ Existing analysis found: {len(ppu_analysis)} PPUs\")\n",
        "        print(f\"\\n  üìä PPU Summary:\")\n",
        "        for _, row in ppu_analysis.head(5).iterrows():\n",
        "            print(f\"     {row['PPU']:<15} Eff: {row['Efficiency']*100:5.1f}%  LCOE: {row['LCOE_CHF_kWh']:.4f} CHF/kWh\")\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è  Run analyze_ppu_chains.py to generate {ppu_analysis_output.name}\")\n",
        "\n",
        "print(\"\\n‚úÖ PPU cost data processing complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Final Validation\n",
        "\n",
        "Check all output files are ready for `Energy_Portfolio_Optimization.ipynb`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 6. FINAL VALIDATION\n",
        "# ============================================================================\n",
        "\n",
        "print_section(\"6. FINAL VALIDATION\", char=\"=\", width=80)\n",
        "\n",
        "# Required output files\n",
        "required_files = [\n",
        "    ('solar_incidence_hourly_2024.csv', 'Solar GHI (hourly √ó locations)'),\n",
        "    ('wind_incidence_hourly_2024.csv', 'Wind speed (hourly √ó locations)'),\n",
        "    ('solar_incidence_ranking.csv', 'Solar location ranking'),\n",
        "    ('wind_incidence_ranking.csv', 'Wind location ranking'),\n",
        "    ('spot_price_hourly.csv', 'Spot electricity prices (CHF/MWh)'),\n",
        "    ('chf_to_eur_2024.csv', 'EUR/CHF exchange rates'),\n",
        "    ('monthly_hourly_load_values_2024.csv', 'Electricity demand'),\n",
        "    ('water_quarterly_ror_2024.csv', 'Run-of-river (15-min)'),\n",
        "    ('Swiss_Water_Hourly_2024.csv', 'Water inflow (hourly)'),\n",
        "    ('cost_table_tidy.csv', 'Component cost table'),\n",
        "    ('ppu_constructs_components.csv', 'PPU definitions'),\n",
        "    ('ppu_efficiency_lcoe_analysis.csv', 'PPU efficiency & LCOE'),\n",
        "]\n",
        "\n",
        "print(\"\\nüìã Checking Required Output Files:\\n\")\n",
        "\n",
        "all_ok = True\n",
        "ready_files = []\n",
        "missing_files = []\n",
        "\n",
        "for filename, description in required_files:\n",
        "    path = IMPORT_DIR / filename\n",
        "    if path.exists():\n",
        "        size_mb = path.stat().st_size / (1024 * 1024)\n",
        "        status = f\"‚úÖ {filename:<45} {size_mb:>7.2f} MB\"\n",
        "        print(status)\n",
        "        ready_files.append(filename)\n",
        "    else:\n",
        "        status = f\"‚ùå {filename:<45} MISSING\"\n",
        "        print(status)\n",
        "        missing_files.append(filename)\n",
        "        all_ok = False\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "if all_ok:\n",
        "    print(\"üéâ ALL FILES READY!\")\n",
        "    print(f\"   {len(ready_files)} / {len(required_files)} files present in import/\")\n",
        "    print(\"\\n   ‚úÖ You can now run Energy_Portfolio_Optimization.ipynb\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  {len(missing_files)} / {len(required_files)} FILES MISSING\")\n",
        "    print(f\"\\n   Missing files:\")\n",
        "    for f in missing_files:\n",
        "        print(f\"     - {f}\")\n",
        "    print(f\"\\n   üí° Check the dumping/ folder for required raw data\")\n",
        "    print(f\"      and re-run relevant sections above.\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nüìä PIPELINE SUMMARY:\")\n",
        "print(f\"   Dumping directory:  {DUMPING_DIR.absolute()}\")\n",
        "print(f\"   Import directory:   {IMPORT_DIR.absolute()}\")\n",
        "print(f\"   Plot directory:     {PLOT_DIR.absolute()}\")\n",
        "print(f\"   Files ready:        {len(ready_files)} / {len(required_files)}\")\n",
        "print(f\"   Year processed:     {YEAR}\")\n",
        "print(f\"   Hours in year:      {N_HOURS_YEAR}\")\n",
        "\n",
        "print(\"\\n‚úÖ Data preprocessing pipeline complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appendix: Data Sources\n",
        "\n",
        "Complete list of raw data sources and download instructions.\n",
        "\n",
        "### Data Sources Reference\n",
        "\n",
        "| Dataset | Source | URL | File Format |\n",
        "|---------|--------|-----|-------------|\n",
        "| **Solar** | Open-Meteo ERA5 | https://open-meteo.com/en/docs/historical-weather-api | GRIB |\n",
        "| **Wind** | Open-Meteo ERA5 | https://open-meteo.com/en/docs/historical-weather-api | GRIB |\n",
        "| **Spot Prices** | ENTSO-E Transparency Platform | https://transparency.entsoe.eu/ | JSON |\n",
        "| **EUR/CHF** | European Central Bank | https://www.ecb.europa.eu/ | CSV |\n",
        "| **USD/CHF** | Dukascopy | https://www.dukascopy.com/ | CSV |\n",
        "| **Demand** | ENTSO-E | https://transparency.entsoe.eu/ | CSV |\n",
        "| **RoR Hydro** | Swiss Federal Office of Energy (BFE) | https://www.bfe.admin.ch/ | CSV |\n",
        "| **Water Inflow** | Open-Meteo Precipitation | https://open-meteo.com/ | CSV |\n",
        "| **Palm Oil** | REA Holdings | https://www.rea.co.uk/ | XLS/CSV |\n",
        "| **Cost Table** | Project Analysis | Manual compilation | CSV |\n",
        "\n",
        "### Architecture Benefits\n",
        "\n",
        "This modular design provides:\n",
        "\n",
        "1. **Separation of Concerns**: Raw data (dumping/) vs. processed data (import/)\n",
        "2. **Reproducibility**: Each dataset has dedicated treatment and validation\n",
        "3. **Maintainability**: Functions are reusable and well-documented\n",
        "4. **Traceability**: Each step logs what it does with clear status messages\n",
        "5. **Visualization**: Every dataset includes preprocessing plots for validation\n",
        "6. **Software Engineering**: Type hints, docstrings, error handling, DRY principles\n",
        "\n",
        "### Usage Notes\n",
        "\n",
        "- **First Run**: Ensure all raw data files are in `dumping/` folder before running\n",
        "- **Subsequent Runs**: Notebook intelligently loads existing processed files\n",
        "- **Debugging**: Each section is independent - can be run individually\n",
        "- **Extensibility**: Easy to add new data sources following the same pattern\n",
        "\n",
        "---\n",
        "\n",
        "**Version**: 1.0  \n",
        "**Compatible with**: Energy_Portfolio_Optimization.ipynb  \n",
        "**Python**: 3.9+  \n",
        "**Key Dependencies**: pandas, numpy, scipy, matplotlib, xarray, cfgrib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Preprocessing Plots: Solar Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 1.2 SOLAR: PREPROCESSING PLOTS & ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "if 'solar_df' in globals() and solar_df is not None and len(solar_df) > 0:\n",
        "    print_section(\"1.2 SOLAR DATA ANALYSIS & VISUALIZATION\")\n",
        "    \n",
        "    # Flatten to 1D for statistics\n",
        "    solar_values = solar_df.values.flatten()\n",
        "    solar_values = solar_values[~np.isnan(solar_values)]\n",
        "    \n",
        "    # Compute and print statistics\n",
        "    solar_stats = compute_statistics(solar_values, \"Solar\")\n",
        "    print_statistics(solar_stats, \"Solar Incidence Statistics (All Locations, All Hours)\")\n",
        "    \n",
        "    # Annual total per location\n",
        "    annual_per_loc = solar_df.sum(axis=0)\n",
        "    print(f\"\\nüìä Annual Totals per Location:\")\n",
        "    print(f\"  Best location:  {annual_per_loc.max():.1f} kWh/m¬≤/year\")\n",
        "    print(f\"  Worst location: {annual_per_loc.min():.1f} kWh/m¬≤/year\")\n",
        "    print(f\"  Mean:           {annual_per_loc.mean():.1f} kWh/m¬≤/year\")\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "    \n",
        "    # 1. Monthly average\n",
        "    monthly_avg = solar_df.mean(axis=1).resample('M').mean()\n",
        "    axes[0, 0].bar(monthly_avg.index.month, monthly_avg.values, \n",
        "                   color='orange', alpha=0.7, edgecolor='black')\n",
        "    axes[0, 0].set_xlabel('Month')\n",
        "    axes[0, 0].set_ylabel('Mean Solar Incidence (kWh/m¬≤/hour)')\n",
        "    axes[0, 0].set_title('Monthly Average Solar Incidence (2024)')\n",
        "    axes[0, 0].set_xticks(range(1, 13))\n",
        "    axes[0, 0].set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
        "                                'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # 2. Distribution histogram\n",
        "    axes[0, 1].hist(solar_values[solar_values > 0], bins=50, \n",
        "                    color='orange', alpha=0.7, edgecolor='black')\n",
        "    axes[0, 1].set_xlabel('Solar Incidence (kWh/m¬≤/hour)')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    axes[0, 1].set_title('Distribution of Non-Zero Solar Incidence')\n",
        "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # 3. Random day profile\n",
        "    random_date = pd.Timestamp('2024-07-15')  # Mid-summer day\n",
        "    if random_date in solar_df.index:\n",
        "        day_profile = solar_df.loc[random_date:random_date + pd.Timedelta(days=1)].mean(axis=1)\n",
        "        axes[1, 0].plot(day_profile.index.hour, day_profile.values, \n",
        "                       'o-', linewidth=2, markersize=6, color='orange')\n",
        "        axes[1, 0].fill_between(day_profile.index.hour, day_profile.values, alpha=0.3, color='orange')\n",
        "        axes[1, 0].set_xlabel('Hour of Day')\n",
        "        axes[1, 0].set_ylabel('Average Solar Incidence (kWh/m¬≤/hour)')\n",
        "        axes[1, 0].set_title(f'Daily Profile - {random_date.strftime(\"%Y-%m-%d\")} (Average Across Locations)')\n",
        "        axes[1, 0].set_xlim(0, 23)\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Annual heatmap (monthly x location percentiles)\n",
        "    monthly_data = []\n",
        "    for month in range(1, 13):\n",
        "        month_data = solar_df[solar_df.index.month == month].mean(axis=0)\n",
        "        monthly_data.append([\n",
        "            month_data.quantile(0.1),\n",
        "            month_data.quantile(0.25),\n",
        "            month_data.quantile(0.5),\n",
        "            month_data.quantile(0.75),\n",
        "            month_data.quantile(0.9)\n",
        "        ])\n",
        "    \n",
        "    heatmap_data = np.array(monthly_data).T\n",
        "    im = axes[1, 1].imshow(heatmap_data, aspect='auto', cmap='YlOrRd', origin='lower')\n",
        "    axes[1, 1].set_xticks(range(12))\n",
        "    axes[1, 1].set_xticklabels(['J', 'F', 'M', 'A', 'M', 'J', 'J', 'A', 'S', 'O', 'N', 'D'])\n",
        "    axes[1, 1].set_yticks(range(5))\n",
        "    axes[1, 1].set_yticklabels(['P10', 'P25', 'P50', 'P75', 'P90'])\n",
        "    axes[1, 1].set_xlabel('Month')\n",
        "    axes[1, 1].set_ylabel('Location Percentile')\n",
        "    axes[1, 1].set_title('Solar Incidence: Monthly Variation by Location Quality')\n",
        "    plt.colorbar(im, ax=axes[1, 1], label='kWh/m¬≤/hour')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    save_plot(fig, '01_solar_analysis.png')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ Solar data processing complete!\")\n",
        "    \n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Solar data not available - skipping analysis\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
